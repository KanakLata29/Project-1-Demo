{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIF501K9LOwjZ+T5QYP0cx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KanakLata29/Project-1-Demo/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nK0Yq8mtTJip",
        "outputId": "c4f5f6ac-a398-4926-9967-9d3a50703f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] --data DATA [--target TARGET] [--out OUT]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --data\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/argparse.py\", line 1943, in _parse_known_args2\n",
            "    namespace, args = self._parse_known_args(args, namespace, intermixed)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/argparse.py\", line 2230, in _parse_known_args\n",
            "    raise ArgumentError(None, _('the following arguments are required: %s') %\n",
            "argparse.ArgumentError: the following arguments are required: --data\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-4186512241.py\", line 359, in <cell line: 0>\n",
            "    main()\n",
            "  File \"/tmp/ipython-input-4186512241.py\", line 350, in main\n",
            "    args = parse_args()\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4186512241.py\", line 347, in parse_args\n",
            "    return parser.parse_args()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/argparse.py\", line 1904, in parse_args\n",
            "    args, argv = self.parse_known_args(args, namespace)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/argparse.py\", line 1914, in parse_known_args\n",
            "    return self._parse_known_args2(args, namespace, intermixed=False)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/argparse.py\", line 1945, in _parse_known_args2\n",
            "    self.error(str(err))\n",
            "  File \"/usr/lib/python3.12/argparse.py\", line 2650, in error\n",
            "    self.exit(2, _('%(prog)s: error: %(message)s\\n') % args)\n",
            "  File \"/usr/lib/python3.12/argparse.py\", line 2637, in exit\n",
            "    _sys.exit(status)\n",
            "SystemExit: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1701, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/argparse.py\u001b[0m in \u001b[0;36m_parse_known_args2\u001b[0;34m(self, args, namespace, intermixed)\u001b[0m\n\u001b[1;32m   1942\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1943\u001b[0;31m                 \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1944\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/argparse.py\u001b[0m in \u001b[0;36m_parse_known_args\u001b[0;34m(self, arg_strings, namespace, intermixed)\u001b[0m\n\u001b[1;32m   2229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequired_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m             raise ArgumentError(None, _('the following arguments are required: %s') %\n\u001b[0m\u001b[1;32m   2231\u001b[0m                        ', '.join(required_actions))\n",
            "\u001b[0;31mArgumentError\u001b[0m: the following arguments are required: --data",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4186512241.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4186512241.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4186512241.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Output folder (overrides config).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1904\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1905\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_known_args2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/argparse.py\u001b[0m in \u001b[0;36m_parse_known_args2\u001b[0;34m(self, args, namespace, intermixed)\u001b[0m\n\u001b[1;32m   1944\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2650\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.12/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2636\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2637\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import warnings\n",
        "from typing import Tuple, List\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG - change to match your dataset\n",
        "# ----------------------------\n",
        "CONFIG = {\n",
        "    \"target\": \"price_usd\",          # target column name in CSV\n",
        "    \"id_col\": None,                 # optional ID column to drop, e.g. 'model_id'\n",
        "    \"datetime_cols\": [],            # e.g. ['release_date'] (not used by default)\n",
        "    # Known features expected in dataset (will try to use if present)\n",
        "    \"expected_numeric\": [\n",
        "        \"memory_gb\", \"ram_gb\", \"battery_mah\",\n",
        "        \"rear_cam_mp\", \"front_cam_mp\", \"height_mm\"\n",
        "    ],\n",
        "    \"expected_categorical\": [\n",
        "        \"model\", \"color\", \"processor\", \"ai_lens\"\n",
        "    ],\n",
        "    # For high-cardinality categorical columns (like model), we'll frequency-encode if > `ohe_threshold`\n",
        "    \"onehot_threshold\": 25,\n",
        "    # Output folder\n",
        "    \"out_dir\": \"outputs\"\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities\n",
        "# ----------------------------\n",
        "def ensure_out_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def load_data(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    return df\n",
        "\n",
        "# ----------------------------\n",
        "# Feature engineering / preprocessing builder\n",
        "# ----------------------------\n",
        "def build_preprocessor(df: pd.DataFrame, cfg: dict) -> Tuple[ColumnTransformer, List[str]]:\n",
        "    \"\"\"\n",
        "    Build a ColumnTransformer that:\n",
        "    - imputes numeric columns (median) + scales (StandardScaler)\n",
        "    - imputes categorical (most frequent) + OneHotEncode (or frequency encode for high-cardinality)\n",
        "    Returns (preprocessor, output_feature_names) where feature names are approximate.\n",
        "    \"\"\"\n",
        "    num_cols = [c for c in cfg[\"expected_numeric\"] if c in df.columns]\n",
        "    cat_cols = [c for c in cfg[\"expected_categorical\"] if c in df.columns]\n",
        "\n",
        "    # If dataset contains other categorical columns, add them\n",
        "    other_cat = [c for c in df.columns if df[c].dtype == \"object\" and c not in cat_cols]\n",
        "    # Avoid including the target or ID\n",
        "    other_cat = [c for c in other_cat if c != cfg[\"target\"] and c != cfg[\"id_col\"]]\n",
        "    cat_cols += other_cat\n",
        "\n",
        "    # Decide which categorical cols get one-hot vs frequency encoding\n",
        "    onehot_cols, freq_cols = [], []\n",
        "    for c in cat_cols:\n",
        "        n_unique = df[c].nunique(dropna=True)\n",
        "        if n_unique <= cfg[\"onehot_threshold\"]:\n",
        "            onehot_cols.append(c)\n",
        "        else:\n",
        "            freq_cols.append(c)\n",
        "\n",
        "    transformers = []\n",
        "    feature_names = []\n",
        "\n",
        "    # Numeric pipeline\n",
        "    if num_cols:\n",
        "        num_pipeline = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"scaler\", StandardScaler()),\n",
        "        ])\n",
        "        transformers.append((\"num\", num_pipeline, num_cols))\n",
        "        feature_names += num_cols\n",
        "\n",
        "    # One-hot categorical pipeline\n",
        "    if onehot_cols:\n",
        "        cat_pipeline = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
        "        ])\n",
        "        transformers.append((\"cat_ohe\", cat_pipeline, onehot_cols))\n",
        "        # We'll build names later once fitted.\n",
        "\n",
        "    # Frequency (count) encoding -> implemented as a transformer using pandas in fit_transform wrapper\n",
        "    # We'll implement a simple custom transformer for freq-encoding below\n",
        "    if freq_cols:\n",
        "        # For frequency encoding we'll just encode into new numeric columns before ColumnTransformer\n",
        "        pass\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\", sparse_threshold=0)\n",
        "\n",
        "    return preprocessor, num_cols, onehot_cols, freq_cols\n",
        "\n",
        "# Simple helper to frequency-encode specified columns (inplace copy)\n",
        "def add_frequency_encoding(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in cols:\n",
        "        freq = df[c].value_counts(dropna=False).to_dict()\n",
        "        df[f\"{c}__freq\"] = df[c].map(freq).astype(float)\n",
        "    return df\n",
        "\n",
        "# ----------------------------\n",
        "# Modeling & evaluation helpers\n",
        "# ----------------------------\n",
        "def evaluate_model(model, X_test, y_test) -> dict:\n",
        "    preds = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
        "    r2 = r2_score(y_test, preds)\n",
        "    return {\"mae\": mae, \"rmse\": rmse, \"r2\": r2, \"preds\": preds}\n",
        "\n",
        "# ----------------------------\n",
        "# Main pipeline\n",
        "# ----------------------------\n",
        "def run_pipeline(csv_path: str, cfg: dict):\n",
        "    out_dir = cfg[\"out_dir\"]\n",
        "    ensure_out_dir(out_dir)\n",
        "\n",
        "    print(f\"Loading data from: {csv_path}\")\n",
        "    df = load_data(csv_path)\n",
        "    print(\"Initial shape:\", df.shape)\n",
        "\n",
        "    # Basic cleaning: drop ID, drop rows missing target\n",
        "    if cfg[\"id_col\"] and cfg[\"id_col\"] in df.columns:\n",
        "        df = df.drop(columns=[cfg[\"id_col\"]])\n",
        "    if cfg[\"target\"] not in df.columns:\n",
        "        raise ValueError(f\"Target column '{cfg['target']}' not found in dataset.\")\n",
        "\n",
        "    df = df.dropna(subset=[cfg[\"target\"]])\n",
        "    print(\"After dropping rows without target:\", df.shape)\n",
        "\n",
        "    # Derived features (if original columns exist)\n",
        "    if {\"rear_cam_mp\", \"front_cam_mp\"}.issubset(df.columns):\n",
        "        df[\"camera_total_mp\"] = df[\"rear_cam_mp\"].fillna(0) + df[\"front_cam_mp\"].fillna(0)\n",
        "    if {\"memory_gb\", \"ram_gb\"}.issubset(df.columns):\n",
        "        df[\"mem_to_ram_ratio\"] = df[\"memory_gb\"] / (df[\"ram_gb\"].replace(0, np.nan))\n",
        "\n",
        "    # Frequency-encode high-cardinality categorical features\n",
        "    preprocessor, num_cols, onehot_cols, freq_cols = build_preprocessor(df, cfg)\n",
        "    if freq_cols:\n",
        "        df = add_frequency_encoding(df, freq_cols)\n",
        "        # After encoding, treat these new numeric columns as numeric features\n",
        "        freq_cols_encoded = [f\"{c}__freq\" for c in freq_cols]\n",
        "        num_cols += freq_cols_encoded\n",
        "\n",
        "    # Final check for numeric columns present\n",
        "    num_cols = [c for c in num_cols if c in df.columns]\n",
        "    onehot_cols = [c for c in onehot_cols if c in df.columns]\n",
        "\n",
        "    # Compose full preprocessor now (rebuild to include new numerics)\n",
        "    transformers = []\n",
        "    if num_cols:\n",
        "        num_pipeline = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"scaler\", StandardScaler())\n",
        "        ])\n",
        "        transformers.append((\"num\", num_pipeline, num_cols))\n",
        "    if onehot_cols:\n",
        "        cat_pipeline = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
        "        ])\n",
        "        transformers.append((\"cat_ohe\", cat_pipeline, onehot_cols))\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\", sparse_threshold=0)\n",
        "\n",
        "    # Prepare X and y\n",
        "    y = df[cfg[\"target\"]].astype(float)\n",
        "    X = df.drop(columns=[cfg[\"target\"]])\n",
        "\n",
        "    # Train/test split\n",
        "    X_train_df, X_test_df, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "    print(\"Train/test sizes:\", X_train_df.shape, X_test_df.shape)\n",
        "\n",
        "    # Fit preprocessor\n",
        "    X_train = preprocessor.fit_transform(X_train_df)\n",
        "    X_test = preprocessor.transform(X_test_df)\n",
        "\n",
        "    # Build feature names list (approximate)\n",
        "    feature_names = []\n",
        "    feature_names += num_cols\n",
        "    if onehot_cols:\n",
        "        ohe = preprocessor.named_transformers_[\"cat_ohe\"].named_steps[\"onehot\"]\n",
        "        ohe_names = list(ohe.get_feature_names_out(onehot_cols))\n",
        "        feature_names += ohe_names\n",
        "\n",
        "    # ----------------------------\n",
        "    # Feature analysis: correlation (numeric) and mutual information\n",
        "    # ----------------------------\n",
        "    numeric_for_corr = [c for c in num_cols if c in df.columns]\n",
        "    corr_series = None\n",
        "    if numeric_for_corr:\n",
        "        corr_series = df[numeric_for_corr + [cfg[\"target\"]]].corr()[cfg[\"target\"]].sort_values(ascending=False)\n",
        "        corr_series.to_csv(os.path.join(out_dir, \"correlation_with_target.csv\"))\n",
        "        print(\"\\nTop correlations (numeric):\\n\", corr_series.head(10))\n",
        "\n",
        "    # Mutual information (non-linear) on preprocessed features\n",
        "    mi = mutual_info_regression(X_train, y_train, random_state=42)\n",
        "    mi_series = pd.Series(mi, index=feature_names).sort_values(ascending=False)\n",
        "    mi_series.head(15).to_csv(os.path.join(out_dir, \"mutual_info_top15.csv\"))\n",
        "    print(\"\\nTop mutual information features:\\n\", mi_series.head(10))\n",
        "\n",
        "    # PCA (to understand explained variance)\n",
        "    try:\n",
        "        pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
        "        pca.fit(X_train)\n",
        "        explained_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "        # Save explained variance to CSV\n",
        "        pd.Series(explained_var, index=[f\"PC{i+1}\" for i in range(len(explained_var))]).to_csv(\n",
        "            os.path.join(out_dir, \"pca_explained_variance.csv\")\n",
        "        )\n",
        "    except Exception:\n",
        "        explained_var = None\n",
        "\n",
        "    # ----------------------------\n",
        "    # Models: baseline training\n",
        "    # ----------------------------\n",
        "    models = {\n",
        "        \"LinearRegression\": LinearRegression(),\n",
        "        \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
        "        \"GradientBoosting\": GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
        "    }\n",
        "\n",
        "    eval_results = {}\n",
        "    trained_models = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name} ...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        metrics = evaluate_model(model, X_test, y_test)\n",
        "        eval_results[name] = {\"mae\": metrics[\"mae\"], \"rmse\": metrics[\"rmse\"], \"r2\": metrics[\"r2\"]}\n",
        "        trained_models[name] = model\n",
        "        print(f\"{name} -> MAE: {metrics['mae']:.2f}, RMSE: {metrics['rmse']:.2f}, R2: {metrics['r2']:.3f}\")\n",
        "\n",
        "    # Save evaluation summary\n",
        "    eval_df = pd.DataFrame(eval_results).T.sort_values(\"mae\")\n",
        "    eval_df.to_csv(os.path.join(out_dir, \"model_evaluation.csv\"))\n",
        "\n",
        "    # ----------------------------\n",
        "    # Feature importances from RandomForest + permutation importance\n",
        "    # ----------------------------\n",
        "    rf = trained_models.get(\"RandomForest\")\n",
        "    if rf is not None:\n",
        "        try:\n",
        "            importances = pd.Series(rf.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "            importances.head(30).to_csv(os.path.join(out_dir, \"rf_feature_importances_top30.csv\"))\n",
        "            # Permutation importance\n",
        "            perm = permutation_importance(rf, X_test, y_test, n_repeats=20, random_state=42, n_jobs=-1)\n",
        "            perm_imp = pd.Series(perm.importances_mean, index=feature_names).sort_values(ascending=False)\n",
        "            perm_imp.head(30).to_csv(os.path.join(out_dir, \"perm_importances_top30.csv\"))\n",
        "            print(\"\\nTop RandomForest importances:\\n\", importances.head(10))\n",
        "            print(\"\\nTop permutation importances:\\n\", perm_imp.head(10))\n",
        "        except Exception as e:\n",
        "            print(\"Could not compute feature importances:\", e)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Save artifacts: preprocessor + best model (by MAE)\n",
        "    # ----------------------------\n",
        "    best_model_name = eval_df.index[0]\n",
        "    best_model = trained_models[best_model_name]\n",
        "\n",
        "    artifact_preprocessor_path = os.path.join(out_dir, \"preprocessor.joblib\")\n",
        "    artifact_model_path = os.path.join(out_dir, f\"best_model_{best_model_name}.joblib\")\n",
        "    joblib.dump(preprocessor, artifact_preprocessor_path)\n",
        "    joblib.dump(best_model, artifact_model_path)\n",
        "    print(f\"\\nSaved preprocessor -> {artifact_preprocessor_path}\")\n",
        "    print(f\"Saved best model ({best_model_name}) -> {artifact_model_path}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Plots\n",
        "    # ----------------------------\n",
        "    # 1) Correlation bar (numeric)\n",
        "    if corr_series is not None:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        corr_series.sort_values(ascending=False).plot(kind=\"bar\")\n",
        "        plt.title(\"Correlation with target (numeric features)\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(out_dir, \"correlation_with_target.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # 2) Mutual info top features\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    mi_series.head(15).plot(kind=\"bar\")\n",
        "    plt.title(\"Top 15 features by mutual information\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir, \"mutual_info_top15.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 3) PCA explained variance\n",
        "    if explained_var is not None:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.plot(np.arange(1, len(explained_var) + 1), explained_var, marker=\"o\")\n",
        "        plt.xlabel(\"Number of PCA components\")\n",
        "        plt.ylabel(\"Cumulative explained variance\")\n",
        "        plt.title(\"PCA cumulative explained variance\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(out_dir, \"pca_explained_variance.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # 4) RF feature importances (top 20)\n",
        "    try:\n",
        "        if rf is not None:\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            importances.head(20).sort_values().plot(kind=\"barh\")\n",
        "            plt.title(\"RandomForest top 20 feature importances\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(out_dir, \"rf_top20_importances.png\"))\n",
        "            plt.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Final summary saved\n",
        "    print(\"\\nPipeline completed. Outputs saved in:\", out_dir)\n",
        "    print(\"Key files:\")\n",
        "    for fname in sorted(os.listdir(out_dir)):\n",
        "        print(\" -\", fname)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# CLI\n",
        "# ----------------------------\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Mobile Phone Price Prediction Pipeline\")\n",
        "    parser.add_argument(\"--data\", type=str, required=True, help=\"Path to CSV file containing the data.\")\n",
        "    parser.add_argument(\"--target\", type=str, default=None, help=\"Name of the target column (overrides config).\")\n",
        "    parser.add_argument(\"--out\", type=str, default=None, help=\"Output folder (overrides config).\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    cfg = CONFIG.copy()\n",
        "    if args.target:\n",
        "        cfg[\"target\"] = args.target\n",
        "    if args.out:\n",
        "        cfg[\"out_dir\"] = args.out\n",
        "    run_pipeline(args.data, cfg)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace '/path/to/your/data.csv' with the actual path to your data file\n",
        "    data_path = '/path/to/your/data.csv'\n",
        "    run_pipeline(data_path, CONFIG)"
      ]
    }
  ]
}